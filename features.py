# -*- coding: utf-8 -*-
"""features.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBX1C3KMByw6VJ_rF7d5_PL2SNQlQgi_
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

# enter the foldername in your Drive where you have saved the unzipped
# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'
# folders.
# e.g. 'cs231n/assignments/assignment1/cs231n/'
FOLDERNAME = 'cs231n/assignments/assignment1/cs231n/'

assert FOLDERNAME is not None, "[!] Enter the foldername."

# %cd drive/My\ Drive
# %cp -r $FOLDERNAME ../../
# %cd ../../
# %cd cs231n/datasets/
!bash get_datasets.sh
# %cd ../../

"""이 과제에서 시사하는 것은
1. neural network를 사용했을 때 linear classifier보다 성능이 좋다.
2. raw pixel을 가지고 train시키는 것보다 feature vector를 추출해서 train시키는 것이 더 성능이 좋다.

# Image features exercise
*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*

We have seen that we can achieve reasonable performance on an image classification task by training a linear classifier on the pixels of the input image. **In this exercise we will show that we can improve our classification performance by training linear classifiers not on raw pixels but on features that are computed from the raw pixels.** All of your work for this exercise will be done in this notebook.  
이전에는 입력 이미지의 raw pixel 값에 대해 linear classifier를 훈련시킴으로써 이미지 분류 작업에서 나름 합리적인 성능을 달성할 수 있다는 것을 보았다.
linear classifier에 raw pixel 값들을 인풋으로 집어넣으면 classifier가 제대로 작동하지 않는다. 픽셀에서 추출한 feature vector를 이용해서 linear classifiers을 training 시켜 분류 성능을 향상시킬 수 있음을 보여줄 것이다. feature값을 color histogram으로 color spectrum을 확인해볼 것이다.
"""

# Commented out IPython magic to ensure Python compatibility.
import random
import numpy as np
from cs231n.data_utils import load_CIFAR10
import matplotlib.pyplot as plt
# 랜덤 모듈, 넘파이, 맷플롯립 패키지 모두 임포트

# %matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # reParams 설정에서 plot의 크기 설정
plt.rcParams['image.interpolation'] = 'nearest'
# nearest 보간 방식을 사용한다 - 이게 nearest neighbor interpolation이랑 같은 건가?
# nearest 보간 방식: 새로운 지점 또는 한 지점 값을 결정할 때 주변 분포한 값을 사용해 결정하는 것
plt.rcParams['image.cmap'] = 'gray' #회색조 이미지를 표시하기 위해 매개변수 cmap을 gray로 설정


# for auto-reloading extenrnal modules
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2

"""## Load data
Similar to previous exercises, we will load CIFAR-10 data from disk.  
이번에도 CIFAR-10 데이터를 이용할 것이다.
"""

from cs231n.features import color_histogram_hsv, hog_feature

def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):
    # 49000개의 training data, 1000개의 validation set, 1000개의 test set
    # Load the raw CIFAR-10 data
    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'

    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)
    try:
       del X_train, y_train
       del X_test, y_test
       print('Clear previously loaded data.')
    except:
       pass

    # https://leechamin.tistory.com/261 히스토그램 내용
    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)
    # X_train : feature 데이터(모델 훈련시킬 때 사용함)
    # y_train : feature의 타깃 데이터(모델 훈련시킬 때 사용함)
    # Subsample the data
    print(len(X_train)) #50000개(train+val)의 데이터가 저장됨??이게 맞는 건가?
    print(len(X_test))  #10000개의 데이터가 저장됨?? 이 정보는 어디서 확인할 수 있지??
    print(len(y_train)) #50000개
    print(len(y_test))  #10000개 

    mask = list(range(num_training, num_training + num_validation)) #49000~49999까지의 1000개의 숫자가 mask 리스트에 저장되어 mask 이미지 생성
    X_val = X_train[mask]
    y_val = y_train[mask]
    print(len(X_val)) #1000개
    print(len(y_val)) #1000개

    mask = list(range(num_training))
    X_train = X_train[mask]
    y_train = y_train[mask]
    # print(len(X_train)) 49000개
    # print(len(y_train)) 49000개

    mask = list(range(num_test))
    X_test = X_test[mask]
    y_test = y_test[mask]
    # print(len(X_test)) 1000개
    # print(len(y_test)) 1000개

    # validation, training, test set에 모두 데이터 저장함
    return X_train, y_train, X_val, y_val, X_test, y_test

X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()

"""## Extract Features
For each image we will compute a Histogram of Oriented
Gradients (HOG) as well as a color histogram using the hue channel in HSV
color space. We form our final feature vector for each image by concatenating
the HOG and color histogram feature vectors.  
각 이미지에 대해 HSV 색 공간의 색조 채널을 사용하여 color HOG와 color 히스토그램을 계산한다. HOG와 color 히스토그램의 feature 벡터를 연결하여 각 이미지에 대한 최종 feature 벡터를 구할 것이다.  
  
Roughly speaking, HOG should capture the texture of the image while ignoring
color information, and the color histogram represents the color of the input
image while ignoring texture. As a result, we expect that using both together
ought to work better than using either alone. Verifying this assumption would
be a good thing to try for your own interest.  
HOG는 색에 대한 정보는 무시하고 이미지의 texture를 캡쳐해야 하고, color 히스토그램은 texture는 무시하고 이미지의 색상을 나타낸다. 결과적으로, 둘다 함께 사용하는 것이 둘 중 하나만 사용하는 것보다 효과적이어야 한다고 기대한다.  
  
The `hog_feature` and `color_histogram_hsv` functions both operate on a single
image and return a feature vector for that image. The extract_features
function takes a set of images and a list of feature functions and evaluates
each feature function on each image, storing the results in a matrix where
each column is the concatenation of all feature vectors for a single image.
하나의 이미지에 대해 `hog_feature` and `color_histogram_hsv` 함수 모두 작동하고 그 이미지에 대한 feature vector를 리턴한다. extract_features
function은 이미지 셋과 feature function의 리스트를 취해서 각 이미지에 대한 feature function을 평가하고, 각각의 이미지에 대한 feature vector를 모두 연결한 것을 하나의 column으로 가지는 matrix를 결과로 저장한다.  
  
feature vector 추출하는 방법에 대해 좀 더 직관적으로 이해하고 싶은데...
"""

from cs231n.features import *

num_color_bins = 10 # Number of bins in the color histogram
feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]
X_train_feats = extract_features(X_train, feature_fns, verbose=True)
X_val_feats = extract_features(X_val, feature_fns)
X_test_feats = extract_features(X_test, feature_fns)

# 평균을 빼서 zero centering을 해주고 표준편차로 나눠서 normalize해주는 가장 일반적인 데이터 전처리 방법
# Preprocessing: Subtract the mean feature
# 데이터 전처리1: 평균 차감법
mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)
X_train_feats -= mean_feat
X_val_feats -= mean_feat 
X_test_feats -= mean_feat
# Q) 여기서 X_val_feats의 평균이 아니라 X_train_feats의 평균을 빼줘도 되나? zero centering이 안 맞지 않나..?
# 일반적으로 train 셋에서 구한 평균을 test 데이터에도 동일하게 적용해줌

# Q) 보통 이미지 데이터에 대해서는 zero centering만 해준다고 했는데 왜 normalize를 하는 거지?
# Preprocessing: Divide by standard deviation. This ensures that each feature
# has roughly the same scale.
# 데이터 전처리2: feature scaling 표준편차로 나눠줌(normalization 정규화)
std_feat = np.std(X_train_feats, axis=0, keepdims=True)
X_train_feats /= std_feat
X_val_feats /= std_feat
X_test_feats /= std_feat

# Preprocessing: Add a bias dimension
# 데이터 전처리3: bias trick
X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])
X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])
X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])

# training data에 대해 모두 데이터 전처리 과정을 거치고, featrure값을 추출함

"""## Train SVM on features
Using the multiclass SVM code developed earlier in the assignment, train SVMs on top of the features extracted above; this should achieve better results than training SVMs directly on top of raw pixels.  
멀티 클래스 SVM 코드를 사용하여 위에서 추출한 feature들을 통해 SVM을 training 시킨다. 이는 raw pixel로 SVM을 training시킨 것보다 결과가 더 좋아야 한다.  
(결과를 비교할 수 있으면 좋을 듯)
"""

# Use the validation set to tune the learning rate and regularization strength
# learning rate과 regularization strength를 튜닝하기 위해 validation set를 이용한다
# 3주차 복습과제 svm.ipynb 참고 - feature를 추출하지 않고 raw pixel값에 대해 training
# 그때의 validation accuracy는 37.9%
# linear classifier를 feature vector를 이용해서 train 시킬 경우 성능이 더 좋아질 것
# raw pixel on linear classifier <vs> features on linear classifier

from cs231n.classifiers.linear_classifier import LinearSVM

learning_rates = [1e-9, 1e-8, 1e-7]
regularization_strengths = [5e4, 5e5, 5e6]
# hyperparameter 설정할 때 grid search 방법을 이용 
# 지난시간에 hyperparameter를 optimize하는 방법으로 random search가 더 적합하다고

results = {}
best_val = -1
best_svm = None

################################################################################
# TODO:                                                                        #
# Use the validation set to set the learning rate and regularization strength. #
# This should be identical to the validation that you did for the SVM; save    #
# the best trained classifer in best_svm. You might also want to play          #
# with different numbers of bins in the color histogram. If you are careful    #
# you should be able to get accuracy of near 0.44 on the validation set.       #
################################################################################
# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

np.random.seed(0)

# 9개의 조합에 대해서 어떤 조합의 성능이 가장 좋은지 cross-validation을 통해 알아냄
grid_search = [ (lr,reg) for lr in learning_rates for reg in regularization_strengths ]

for lr, reg in grid_search:
    # Create SVM model
    svm = LinearSVM()
    
    # Train 단계
    svm.train(X_train_feats, y_train, learning_rate=lr, reg=reg, num_iters=2000,
            batch_size=200, verbose=False)
    y_train_pred = svm.predict(X_train_feats)
    # Train accuracy
    train_accuracy = np.mean( y_train_pred == y_train )
    
    # Validation 단계
    y_val_pred = svm.predict(X_val_feats)
    # Validation accuracy
    val_accuracy = np.mean( y_val_pred == y_val )
    
    results[lr,reg] = (train_accuracy,val_accuracy)
    
    # Save best model 가장 좋은 성능을 보인 조합을 저장
    if val_accuracy > best_val:
        best_val = val_accuracy
        best_svm = svm

# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

# Print out results.
for lr, reg in sorted(results):
    train_accuracy, val_accuracy = results[(lr, reg)]
    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (
                lr, reg, train_accuracy, val_accuracy))
    
print('best validation accuracy achieved during cross-validation: %f' % best_val)

# Evaluate your trained SVM on the test set: you should be able to get at least 0.40
# test set으로 성능을 평가
y_test_pred = best_svm.predict(X_test_feats)
test_accuracy = np.mean(y_test == y_test_pred)
print(test_accuracy)

# An important way to gain intuition about how an algorithm works is to
# visualize the mistakes that it makes. In this visualization, we show examples
# of images that are misclassified by our current system. The first column
# shows images that our system labeled as "plane" but whose true label is
# something other than "plane".
# 알고리즘에 대한 직관을 얻는 중요한 방법이 알고리즘이 만드는 오류들을 시각화하는 것

examples_per_class = 8
classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for cls, cls_name in enumerate(classes):
    idxs = np.where((y_test != cls) & (y_test_pred == cls))[0]
    idxs = np.random.choice(idxs, examples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)
        plt.imshow(X_test[idx].astype('uint8'))
        plt.axis('off')
        if i == 0:
            plt.title(cls_name)
plt.show()

"""### Inline question 1:
Describe the misclassification results that you see. Do they make sense?


$\color{blue}{\textit Your Answer:}$ 잘 분류되지는 않았지만 말이 되는 경우도 존재한다. 잘 분류되지 않은 경우 클래스의 이미지와 비슷한 가장자리(edges)와 색을 가지고 있다. 개와 고양이는 가장자리의 특징이 비슷한데, 이럴 경우 이미지들이 서로 혼합되어 분류될 수 있다. 자동차와 트럭도 마찬가지 이유로 혼합될 수 있고, 배와 비행기는 바다와 하늘의 배경색이 비슷하기 때문에 각 클래스에 서로 분류될 수 있다. 다만 아예 말이 안 되게 분류된 경우도 있다.(개구리)  
HOG와 color histogram의 조합으로 모든 클래스들을 올바르게 분류할 수 없다. HOG descriptor는 가장자리를 고려하기 때문에 매우 유용하지만, 가장자리가 비슷한 개와 고양이를 구별하기에 충분하지 않다.

## Neural Network on image features
Earlier in this assigment we saw that training a two-layer neural network on raw pixels achieved better classification performance than linear classifiers on raw pixels. In this notebook we have seen that linear classifiers on image features outperform linear classifiers on raw pixels.  
윗부분은 3주차 때 했던 svm 코드(linear classifier)에서 raw pixel값이 아닌 feature를 추출해서 training시킨 것이 더 성능이 좋다는 것을 보여줬다.
  
For completeness, we should also try training a neural network on image features. This approach should outperform all previous approaches: you should easily be able to achieve over 55% classification accuracy on the test set; our best model achieves about 60% classification accuracy.  
더 완벽하게 하기 위해서 이미지의 feature 값을 이용해서 neural network를 training시켜야 한다. 이 접근 방식은 test set에 대해 accuracy가 55% 이상 달성할 수 있어야 한다.
"""

# Preprocessing: Remove the bias dimension
# Make sure to run this cell only ONCE
# 이거 여러 번 실행 시켜서 최종 test accuracy가 59%에서 58%로 감소함...
# 한번만 빼줘서 bias dimension을 빼줘야 되는데 다른 feature vector들도 remove해줘서... column vector 수가 155->152로 줄었어 (원래대로라면 154)
print(X_train_feats.shape)
# X_train_feats = X_train_feats[:, :-1]
# X_val_feats = X_val_feats[:, :-1]
# X_test_feats = X_test_feats[:, :-1]

print(X_train_feats.shape)

from cs231n.classifiers.neural_net import TwoLayerNet

input_dim = X_train_feats.shape[1]
hidden_dim = 500
num_classes = 10

net = TwoLayerNet(input_dim, hidden_dim, num_classes)
best_net = None
# best_val = -1

################################################################################
# TODO: Train a two-layer neural network on image features. You may want to    #
# cross-validate various parameters as in previous sections. Store your best   #
# model in the best_net variable.                                              #
################################################################################
# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

# 위에서 실험한 것과 다른 점
# 1. hyperparameter 생성할 때 random search를 이용한 것 (<-> grid search)
# 2. linear classifier <-> two layer neural net

# hyperparameter 랜덤하게 생성
def generate_random_hyperparams(lr_min, lr_max, reg_min, reg_max, h_min, h_max):
    lr = 10**np.random.uniform(lr_min,lr_max) 
    # 랜덤모듈에서 uniform - 최솟값과 최댓값 사이의 구간에서 균등한 분포를 이루는 랜덤 넘버를 추출
    # 지난시간 가중치 초기화할 때 보통 radom.randn으로 가우시안 정규분포를 따르도록 초기화를 해주었는데,
    # random.uniform 으로 정규분포가 아닌 균일 분포(uniform distribution)로부터 추출된 값으로 초기화 해도 무방함
    reg = 10**np.random.uniform(reg_min,reg_max)
    hidden = np.random.randint(h_min, h_max)
    return lr, reg, hidden

# Use of random search for hyperparameter search
for i in range(20):
    lr, reg, hidden_dim = generate_random_hyperparams(-1, 0, -7, -4, 10, 500)
    # Create a two-layer network
    net = TwoLayerNet(input_dim, hidden_dim, num_classes)
    
    # Train the network
    stats = net.train(X_train_feats, y_train, X_val_feats, y_val,
                num_iters=3000, batch_size=200,
                learning_rate=lr, learning_rate_decay=0.95,
                reg=reg, verbose=False)

    # Predict on the training set
    train_accuracy = (net.predict(X_train_feats) == y_train).mean()
    
    # Predict on the validation set
    val_accuracy = (net.predict(X_val_feats) == y_val).mean()
    
    # Save best values
    if val_accuracy > best_val:
        best_val = val_accuracy
        best_net = net
    
    # Print results
    print('lr %e reg %e hid %d  train accuracy: %f val accuracy: %f' % (
                lr, reg, hidden_dim, train_accuracy, val_accuracy))
print('best validation accuracy achieved: %f' % best_val)

# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

# Run your best neural net classifier on the test set. You should be able
# to get more than 55% accuracy.

test_acc = (best_net.predict(X_test_feats) == y_test).mean()
print(test_acc)

"""---
# IMPORTANT

This is the end of this question. Please do the following:

1. Click `File -> Save` to make sure the latest checkpoint of this notebook is saved to your Drive.
2. Execute the cell below to download the modified `.py` files back to your drive.
"""

import os

FOLDER_TO_SAVE = os.path.join('drive/My Drive/', FOLDERNAME)
FILES_TO_SAVE = []

for files in FILES_TO_SAVE:
  with open(os.path.join(FOLDER_TO_SAVE, '/'.join(files.split('/')[1:])), 'w') as f:
    f.write(''.join(open(files).readlines()))